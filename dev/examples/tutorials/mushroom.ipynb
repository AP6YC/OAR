{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example shows how to do supervised training and testing with START on the UCI Mushroom dataset.\n",
    "The Mushroom dataset is a purely categorical dataset where each feature has entries that are members of different discrete categories.\n",
    "Where this is normally a challenge for other machine learning models due to encoding schemes and considerations, START learns directly on the symbols of the dataset itself.\n",
    "Furthermore, START can use a simple supervised mode to map clusters to supervised categories to allow for training and performance testing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we load some dependencies:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Just load this project\n",
    "using OAR"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The OAR project has an all-in-one function for loading the dataset, parsing it into statements, and inferring the resulting grammar:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OAR.DataSplitGeneric{SubArray{Vector{GSymbol{String}}, 1, Vector{Vector{GSymbol{String}}}, Tuple{Vector{Int64}}, false}, SubArray{Int64, 1, Vector{Int64}, Tuple{Vector{Int64}}, false}}"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "# Point to the relative file location\n",
    "filename = joinpath(\"..\", \"assets\", \"mushrooms.csv\")\n",
    "# All-in-one function\n",
    "fs, bnf = OAR.symbolic_mushroom(filename)\n",
    "typeof(fs)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intializing START"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the grammar and keyword arguments to set the options of the module during initialization:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "START(ProtoNode[], OAR.CFG{String}(N:22, S:22, P:22, T:23), OAR.opts_START\n  rho: Float64 0.6\n  alpha: Float64 0.001\n  beta: Float64 1.0\n  epochs: Int64 1\n  terminated: Bool false\n, Int64[], Float64[], Float64[], Dict{String, Any}(\"n_categories\" => 0, \"n_clusters\" => 0, \"n_instance\" => Int64[]))"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "# Initialize the module with options\n",
    "art = OAR.START(bnf,\n",
    "    rho = 0.6,\n",
    ")"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could also set or change the options after initialization with `art.opts.rho = 0.7`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To train the model we will use the training statements portion of the dataset that we loaded earlier along with their corresponding supervisory labels:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Iterate over the training data\n",
    "for ix in eachindex(fs.train_x)\n",
    "    statement = fs.train_x[ix]\n",
    "    label = fs.train_y[ix]\n",
    "    OAR.train!(\n",
    "        art,\n",
    "        statement,\n",
    "        y=label,\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "To test the model, we use the testing data and extract the prescribed label for each sample by the model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Create a container for the output labels\n",
    "clusters = zeros(Int, length(fs.test_y))\n",
    "# Iterate over the testing data\n",
    "for ix in eachindex(fs.test_x)\n",
    "    clusters[ix] = OAR.classify(\n",
    "        art,\n",
    "        fs.test_x[ix],\n",
    "        get_bmu=true,\n",
    "    )\n",
    "end"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can finally test the performance of the module by seeing the percentage of testing samples that are incorrectly labeled:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Final performance: 0.9146491588018055\n",
      "[ Info: n_categories: 540\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "# Calculate testing performance\n",
    "perf = OAR.AdaptiveResonance.performance(fs.test_y, clusters)\n",
    "\n",
    "# Logging\n",
    "@info \"Final performance: $(perf)\"\n",
    "@info \"n_categories: $(art.stats[\"n_categories\"])\""
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.4"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.4",
   "language": "julia"
  }
 },
 "nbformat": 4
}
